{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计划\n",
    "\n",
    "第一步，先干两个事，第一个把CIFAR10数据导出来，然后可视化，第二个，把VGG16参数导进来，然后看看数据格式。\n",
    "\n",
    "第二步，试着搭个tensorflow框架，能跑起来，看看还有什么难点没有解决。\n",
    "\n",
    "第三步，添加记录，能够利用tensorboard查看优化过程。\n",
    "\n",
    "第四步，调参，看各个超参数如何影响结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 128\n",
    "IMG_hight = 32\n",
    "IMG_width = 32\n",
    "Channels = 3\n",
    "Classes = 10\n",
    "Learning_rate = 0.01\n",
    "Max_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用队列从文件中直接读取数据\n",
    "\n",
    "1 data_dir要严格采用给出格式，'/'不可以\n",
    "\n",
    "2 这个例程只针对cifar-10-batches-bin二进制文件，py数据集不支持\n",
    "\n",
    "3 注意函数中对图像数据标准化的指令，会导致看到的图像不正常，暂时没找到替代函数放到train()中去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar10(data_dir, is_train, batch_size):\n",
    "    Label_bytes = 1\n",
    "    IMG_hight = 32\n",
    "    IMG_width = 32\n",
    "    Channels = 3\n",
    "    IMG_bytes = IMG_hight * IMG_width * Channels\n",
    "    Classes = 10\n",
    "    \n",
    "    # convert data_dir to tensor\n",
    "    if is_train:\n",
    "        filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)for i in range(1, 6)]\n",
    "    else:\n",
    "        filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    \n",
    "    # define a reader\n",
    "    reader = tf.FixedLengthRecordReader(Label_bytes+IMG_bytes)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "    label = tf.slice(record_bytes, [0], [Label_bytes])\n",
    "    image = tf.slice(record_bytes, [Label_bytes], [IMG_bytes])\n",
    "    image = tf.reshape(image, [Channels, IMG_hight, IMG_width])\n",
    "    image = tf.transpose(image, [1, 2, 0])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    #  Linearly scales image to have zero mean and unit norm\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image, label_batch = tf.train.shuffle_batch([image, label], batch_size=batch_size,\n",
    "                                                num_threads=64, capacity=2000,\n",
    "                                                min_after_dequeue=300)\n",
    "    label_batch = tf.one_hot(label_batch, depth=Classes)\n",
    "    label_batch = tf.cast(label_batch, tf.int32)\n",
    "    label_batch = tf.reshape(label_batch, [batch_size, Classes])\n",
    "    return image, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_read_cifar10():\n",
    "    data_dir = \".//data//cifar-10-batches-bin//\"\n",
    "    Batch_size = 128\n",
    "    is_train = True\n",
    "\n",
    "    img_batch, label_batch = read_cifar10(data_dir=data_dir, is_train=is_train, batch_size=Batch_size)\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        images, labels = sess.run([img_batch, label_batch])\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            plt.imshow(images[i].astype('uint8'))\n",
    "            plt.axis('off')\n",
    "        plt.show\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "# test_read_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建VGG16网络\n",
    "\n",
    "使用tensorflow搭建VGG16网络结构，注意tf.layers是tf.nn的高阶封装，不能导入已训练参数，这里选用tf.nn。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[3,3], strides=[1,1,1,1]):\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights', shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(name='biases', shape=[out_channels], \n",
    "                           initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, strides, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "    return x\n",
    "\n",
    "\n",
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    shape = x.get_shape()\n",
    "    if len(shape) == 4:\n",
    "        size = shape[1] * shape[2] * shape[3]\n",
    "    else:\n",
    "        size = shape[1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights', shape=[size, out_nodes],\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(name='biases', shape=[out_nodes],\n",
    "                           initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size])\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net_VGG16(x, Classes):\n",
    "    with tf.name_scope('VGG16'):\n",
    "        x = conv('conv1_1', x, 64, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv1_2', x, 64, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool1')\n",
    "\n",
    "        x = conv('conv2_1', x, 128, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv2_2', x, 128, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool2')   \n",
    "\n",
    "        x = conv('conv3_1', x, 256, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv3_2', x, 256, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv3_3', x, 256, kernel_size=[3,3], strides=[1,1,1,1])    \n",
    "        x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool3')\n",
    "\n",
    "        x = conv('conv4_1', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv4_2', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv4_3', x, 512, kernel_size=[3,3], strides=[1,1,1,1])    \n",
    "        x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool4')\n",
    "\n",
    "        x = conv('conv5_1', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv5_2', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "        x = conv('conv5_3', x, 512, kernel_size=[3,3], strides=[1,1,1,1])    \n",
    "        x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool5')\n",
    "\n",
    "        x = FC_layer('fc6', x, out_nodes=4096)\n",
    "        x = FC_layer('fc7', x, out_nodes=4096)\n",
    "        x = FC_layer('fc8', x, out_nodes=Classes)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入Pre_train参数\n",
    "\n",
    "注意tf.get_variable的用法，这个的测试函数中，要调用VGG16，先定义好变量，再利用函数加载参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_skip(data_path, session, skip_layer):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    keys = sorted(data_dict.keys())\n",
    "    for key in keys:\n",
    "        if key not in skip_layer:\n",
    "            with tf.variable_scope(key, reuse=True):\n",
    "                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                    session.run(tf.get_variable(subkey).assign(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_with_skip():\n",
    "    tf.reset_default_graph()\n",
    "    pre_trained_weights = \".//vgg16_pretrain//vgg16.npy\"\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[Batch_size, IMG_width, IMG_hight, Channels])\n",
    "    logits = Net_VGG16(x, Classes)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    load_with_skip(pre_trained_weights, sess, ['fc6', 'fc7', 'fc8'])\n",
    "    \n",
    "    data_dict = np.load(pre_trained_weights, encoding='latin1').item()\n",
    "    keys = sorted(data_dict.keys())\n",
    "    for layer_name in keys:\n",
    "        if layer_name not in ['fc6', 'fc7', 'fc8']:\n",
    "            with tf.variable_scope(layer_name, reuse=True):\n",
    "                w = tf.get_variable('weights')\n",
    "                b = tf.get_variable('biases')\n",
    "                print('\\n')\n",
    "                print(layer_name)\n",
    "                print('weights shape: ', w.shape)\n",
    "                print('biases shape', b.shape)\n",
    "# test_load_with_skip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_compute(logits, labels):\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels, name='cross-entropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "def accuracy_compute(logits, labels):\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct) * 100\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def num_correct_prediction(logits, labels):\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        n_correct = tf.reduce_mean(correct) * 100\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    pre_trained_file = \".//vgg16_pretrain//vgg16.npy\"\n",
    "    data_dir = \".//data//cifar-10-batches-bin\"\n",
    "    log_dir =  \".//logs\"\n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        tra_image_batch, tra_label_batch = read_cifar10(data_dir=data_dir, is_train=True, batch_size=Batch_size)\n",
    "        val_image_batch, val_label_batch = read_cifar10(data_dir=data_dir, is_train=False,batch_size=Batch_size)\n",
    "        \n",
    "    x = tf.placeholder(tf.float32, shape=[Batch_size, IMG_width, IMG_hight, Channels])\n",
    "    tf.summary.image('image', x, 10)  # record the images which are loaded for CIFAR10\n",
    "    y = tf.placeholder(tf.int32, shape=[Batch_size, Classes])\n",
    "    logits = Net_VGG16(x, Classes)\n",
    "    loss = loss_compute(logits, y)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=Learning_rate)\n",
    "    train_op = optimizer.minimize(loss, name='optimize')\n",
    "    accuracy = accuracy_compute(logits, y)\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(init)\n",
    "\n",
    "    load_with_skip(pre_trained_file, sess, ['fc6', 'fc7', 'fc8'])\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    tra_summary_writer = tf.summary.FileWriter(log_dir+'//train', sess.graph)\n",
    "    \n",
    "    images, labels = sess.run([tra_image_batch, tra_label_batch])\n",
    "    \n",
    "    for step in range(Max_step):\n",
    "        summary_str, tra_accuracy, _, tra_loss = sess.run([summary_op, accuracy, train_op, loss],\n",
    "                                             feed_dict={x: images, y: labels})\n",
    "        if step % 20 == 0:\n",
    "            print(tra_loss)\n",
    "            print(tra_accuracy)\n",
    "            tra_summary_writer.add_summary(summary_str, step)\n",
    "            checkpoint_path = os.path.join(log_dir+'//train', 'modle.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./logs/train/modle.ckpt-80\n",
      "Loading success\n",
      "\n",
      "Evaluating...\n",
      "Total testing samples: 4992\n",
      "Total correct predictions: 1092\n",
      "Average accuracy: 21.88%\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    with tf.Graph().as_default():\n",
    "        log_dir = './logs/train/'\n",
    "        test_dir = './data/cifar-10-batches-bin'\n",
    "        n_test = 5000\n",
    "        images, labels = read_cifar10(data_dir=test_dir, is_train=False, batch_size=Batch_size)\n",
    "        logits=Net_VGG16(images, Classes)\n",
    "        correct = num_correct_prediction(logits, labels)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session(config=config)\n",
    "        sess.run(init)\n",
    "        \n",
    "        print('Reading checkpoints...')\n",
    "        ckpt = tf.train.get_checkpoint_state(log_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Loading success')\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            return\n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        print('\\nEvaluating...')\n",
    "        num_step = int(math.floor(n_test/Batch_size))\n",
    "        num_sample = num_step * Batch_size\n",
    "        step = 0\n",
    "        total_correct = 0\n",
    "        while step < num_step and not coord.should_stop():\n",
    "            batch_correct = sess.run(correct)\n",
    "            total_correct += np.sum(batch_correct)\n",
    "            step += 1\n",
    "        print('Total testing samples: %d' % num_sample)\n",
    "        print('Total correct predictions: %d' % total_correct)\n",
    "        print('Average accuracy: %.2f%%' %(100*total_correct/num_sample))\n",
    "        \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
