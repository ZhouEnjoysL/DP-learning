{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计划\n",
    "\n",
    "第一步，先干两个事，第一个把CIFAR10数据导出来，然后可视化，第二个，把VGG16参数导进来，然后看看数据格式。\n",
    "\n",
    "第二步，试着搭个tensorflow框架，能跑起来，看看还有什么难点没有解决。\n",
    "\n",
    "第三步，添加记录，能够利用tensorboard查看优化过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 128\n",
    "IMG_hight = 32\n",
    "IMG_width = 32\n",
    "Channels = 3\n",
    "Classes = 10\n",
    "Learning_rate = 0.01\n",
    "Max_step = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用队列从文件中直接读取数据\n",
    "\n",
    "1 data_dir要严格采用给出格式，'/'不可以\n",
    "\n",
    "2 这个例程只针对cifar-10-batches-bin二进制文件，py数据集不支持\n",
    "\n",
    "3 注意函数中对图像数据标准化的指令，会导致看到的图像不正常，暂时没找到替代函数放到train()中去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar10(data_dir, is_train, batch_size):\n",
    "    Label_bytes = 1\n",
    "    IMG_hight = 32\n",
    "    IMG_width = 32\n",
    "    Channels = 3\n",
    "    IMG_bytes = IMG_hight * IMG_width * Channels\n",
    "    Classes = 10\n",
    "    \n",
    "    # convert data_dir to tensor\n",
    "    if is_train:\n",
    "        filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)for i in range(1, 6)]\n",
    "    else:\n",
    "        filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    \n",
    "    # define a reader\n",
    "    reader = tf.FixedLengthRecordReader(Label_bytes+IMG_bytes)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "    label = tf.slice(record_bytes, [0], [Label_bytes])\n",
    "    image = tf.slice(record_bytes, [Label_bytes], [IMG_bytes])\n",
    "    image = tf.reshape(image, [Channels, IMG_hight, IMG_width])\n",
    "    image = tf.transpose(image, [1, 2, 0])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    #  Linearly scales image to have zero mean and unit norm\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image, label_batch = tf.train.shuffle_batch([image, label], batch_size=batch_size,\n",
    "                                                num_threads=64, capacity=2000,\n",
    "                                                min_after_dequeue=300)\n",
    "    label_batch = tf.one_hot(label_batch, depth=Classes)\n",
    "    label_batch = tf.cast(label_batch, tf.int32)\n",
    "    label_batch = tf.reshape(label_batch, [batch_size, Classes])\n",
    "    return image, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_read_cifar10():\n",
    "    data_dir = \".//data//cifar-10-batches-bin//\"\n",
    "    Batch_size = 128\n",
    "    is_train = True\n",
    "\n",
    "    img_batch, label_batch = read_cifar10(data_dir=data_dir, is_train=is_train, batch_size=Batch_size)\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        images, labels = sess.run([img_batch, label_batch])\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        for i in range(16):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            plt.imshow(images[i].astype('uint8'))\n",
    "            plt.axis('off')\n",
    "        plt.show\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "# test_read_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建VGG16网络\n",
    "\n",
    "使用tensorflow搭建VGG16网络结构，注意tf.layers是tf.nn的高阶封装，不能导入已训练参数，这里选用tf.nn。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[3,3], strides=[1,1,1,1]):\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights', shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(name='biases', shape=[out_channels], \n",
    "                           initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, strides, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "    return x\n",
    "\n",
    "\n",
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    shape = x.get_shape()\n",
    "    if len(shape) == 4:\n",
    "        size = shape[1] * shape[2] * shape[3]\n",
    "    else:\n",
    "        size = shape[1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights', shape=[size, out_nodes],\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(name='biases', shape=[out_nodes],\n",
    "                           initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size])\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net_VGG16(x, Classes):\n",
    "\n",
    "    x = conv('conv1_1', x, 64, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv1_2', x, 64, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool1')\n",
    "    \n",
    "    x = conv('conv2_1', x, 128, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv2_2', x, 128, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool2')   \n",
    "\n",
    "    x = conv('conv3_1', x, 256, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv3_2', x, 256, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv3_3', x, 256, kernel_size=[3,3], strides=[1,1,1,1])    \n",
    "    x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool3')\n",
    "  \n",
    "    x = conv('conv4_1', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv4_2', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv4_3', x, 512, kernel_size=[3,3], strides=[1,1,1,1])    \n",
    "    x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool4')\n",
    "    \n",
    "    x = conv('conv5_1', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv5_2', x, 512, kernel_size=[3,3], strides=[1,1,1,1])\n",
    "    x = conv('conv5_3', x, 512, kernel_size=[3,3], strides=[1,1,1,1])    \n",
    "    x = tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='pool5')\n",
    "    \n",
    "    x = FC_layer('fc6', x, out_nodes=4096)\n",
    "    x = FC_layer('fc7', x, out_nodes=4096)\n",
    "    x = FC_layer('fc8', x, out_nodes=Classes)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入Pre_train参数\n",
    "\n",
    "注意tf.get_variable的用法，这个的测试函数中，要调用VGG16，先定义好变量，再利用函数加载参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_skip(data_path, session, skip_layer):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    keys = sorted(data_dict.keys())\n",
    "    for key in keys:\n",
    "        if key not in skip_layer:\n",
    "            with tf.variable_scope(key, reuse=True):\n",
    "                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                    session.run(tf.get_variable(subkey).assign(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_with_skip():\n",
    "    tf.reset_default_graph()\n",
    "    pre_trained_weights = \".//vgg16_pretrain//vgg16.npy\"\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[Batch_size, IMG_width, IMG_hight, Channels])\n",
    "    logits = Net_VGG16(x, Classes)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    load_with_skip(pre_trained_weights, sess, ['fc6', 'fc7', 'fc8'])\n",
    "    \n",
    "    data_dict = np.load(pre_trained_weights, encoding='latin1').item()\n",
    "    keys = sorted(data_dict.keys())\n",
    "    for layer_name in keys:\n",
    "        if layer_name not in ['fc6', 'fc7', 'fc8']:\n",
    "            with tf.variable_scope(layer_name, reuse=True):\n",
    "                w = tf.get_variable('weights')\n",
    "                b = tf.get_variable('biases')\n",
    "                print('\\n')\n",
    "                print(layer_name)\n",
    "                print('weights shape: ', w.shape)\n",
    "                print('biases shape', b.shape)\n",
    "# test_load_with_skip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_compute(logits, labels):\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels, name='cross-entropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        return loss\n",
    "def accuracy_compute(logits, labels):\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3132644\n",
      "9.375\n",
      "0.9264673\n",
      "80.46875\n",
      "0.002013368\n",
      "100.0\n",
      "0.00059447193\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    pre_trained_file = \".//vgg16_pretrain//vgg16.npy\"\n",
    "    data_dir = \".//data//cifar-10-batches-bin\"\n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        tra_image_batch, tra_label_batch = read_cifar10(data_dir=data_dir, is_train=True, batch_size=Batch_size)\n",
    "        val_image_batch, val_label_batch = read_cifar10(data_dir=data_dir, is_train=False,batch_size=Batch_size)\n",
    "        \n",
    "    x = tf.placeholder(tf.float32, shape=[Batch_size, IMG_width, IMG_hight, Channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[Batch_size, Classes])\n",
    "    logits = Net_VGG16(x, Classes)\n",
    "    loss = loss_compute(logits, y)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=Learning_rate)\n",
    "    train_op = optimizer.minimize(loss, name='optimize')\n",
    "    accuracy = accuracy_compute(logits, y)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(init)\n",
    "\n",
    "    load_with_skip(pre_trained_file, sess, ['fc6', 'fc7', 'fc8'])\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    images, labels = sess.run([tra_image_batch, tra_label_batch])\n",
    "    for step in range(Max_step):\n",
    "        tra_accuracy, _, tra_loss = sess.run([accuracy, train_op, loss], feed_dict={x: images, y: labels})\n",
    "        if step % 50 == 0:\n",
    "            print(tra_loss)\n",
    "            print(tra_accuracy)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)    \n",
    "    sess.close()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
